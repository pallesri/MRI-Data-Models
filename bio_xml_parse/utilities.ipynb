{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for finding all actor matches in the the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findall_actor(row):\n",
    "    \n",
    "    \"\"\"\n",
    "        row: input is row of a dataframe\n",
    "        returns ['with_accurate_actor', 'accurate_actor', 'num_accurate_actor_match']\n",
    "    \"\"\"\n",
    "    act = row['actor']\n",
    "\n",
    "    if act is np.nan:\n",
    "        return [False,np.nan, 0]\n",
    "    else:\n",
    "        if act[-1] in ['(', ')', ']','[']:\n",
    "            #act = r'\\b' + act.replace('(', \"\\(\").replace(')', \"\\)\").replace('[',\"\\[\").replace(']',\"\\]\") + r'\\s'\n",
    "            act = r'\\b' + act.replace('(', \"\\(\").replace(')', \"\\)\").replace('[',\"\\[\").replace(']',\"\\]\")\n",
    "        else:\n",
    "            act = r'\\b' + act.replace('(', \"\\(\").replace(')', \"\\)\").replace('[',\"\\[\").replace(']',\"\\]\") + r'\\b'\n",
    "        text = row['text']\n",
    "        all_match = re.findall(act, text, re.I)\n",
    "        tmp = len(all_match)\n",
    "        if tmp == 0:\n",
    "            return [False,np.nan, 0]\n",
    "        elif len(all_match[0]) == 1:\n",
    "            return [False, np.nan, 0]\n",
    "        else:\n",
    "            return [True,all_match[0],tmp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to get span of actors or hints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span(row, target):\n",
    "    text = row['text']\n",
    "        \n",
    "    if row[target][-1] in ['(', ')', ']','[']:\n",
    "        p = r'\\b' + row[target].replace('(', \"\\(\").replace(')', \"\\)\").replace('[',\"\\[\").replace(']',\"\\]\") + r'\\s'\n",
    "    else:\n",
    "        p = r'\\b' + row[target].replace('(', \"\\(\").replace(')', \"\\)\").replace('[',\"\\[\").replace(']',\"\\]\") + r'\\b'\n",
    "          \n",
    "    #p = row[target].replace('(', \"\\(\").replace(')', \"\\)\").replace('[',\"\\[\").replace(']',\"\\]\") # replace parentheses with \\) or \\]\n",
    "    #m = re.search(p, text, re.I)\n",
    "    #p = ' '.join(p.split())\n",
    "    \n",
    "    m = re.search(' '.join(p.split()), text, re.I)\n",
    "    return list(m.span())\n",
    "\n",
    "# Above function just returns the span of occurance of the actor in the text. Where it starts and where it ends\n",
    "# give it a dataframe and actor column name, it will add four new columns start_actor, end_actor, start_hint, end_hint\n",
    "# usage ex:\n",
    "# tmp[['start_actor','end_actor']] = tmp.apply(get_span, args=['actor'], axis = 1, result_type = 'expand')\n",
    "# tmp[['start_hint','end_hint']] = tmp.apply(get_span, args=['hint'], axis = 1, result_type = 'expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span2(row, actor, num_actm):\n",
    "    \"\"\"\n",
    "    row: row of a data frame\n",
    "    actor: actor\n",
    "    num_actm: num_accurate_actor_match\n",
    "    \n",
    "    returns the span of actor in the text\n",
    "    comapred to the above get_span function, this function can work for multiple same actor occurances, and when we need the \n",
    "    span for only one correct actor occurence.\n",
    "    \"\"\"\n",
    "    text = row['text']\n",
    "    act = row[actor]\n",
    "    #print (text)\n",
    "    #print (act)\n",
    "    #print ('no of actors', row[num_actm])\n",
    "    \n",
    "    hints = ['shall', 'will', 'must', 'is required to', 'are required to']\n",
    "#     num_hints = len(re.findall(ht,text,re.I))\n",
    "#     print ('no of hints', num_hints)\n",
    "    \n",
    "    if(row[num_actm] == 0):\n",
    "        null_pos = [np.nan, np.nan]\n",
    "        return null_pos\n",
    "    \n",
    "    elif(row[num_actm] == 1):\n",
    "        if row[actor][-1] in ['(', ')', ']','[']:\n",
    "            p = r'\\b' + row[actor].replace('(', \"\\(\").replace(')', \"\\)\").replace('[',\"\\[\").replace(']',\"\\]\")\n",
    "        else:\n",
    "            p = r'\\b' + row[actor].replace('(', \"\\(\").replace(')', \"\\)\").replace('[',\"\\[\").replace(']',\"\\]\") + r'\\b'\n",
    "        #p = row[target].replace('(', \"\\(\").replace(')', \"\\)\").replace('[',\"\\[\").replace(']',\"\\]\") # replace parentheses with \\) or \\]\n",
    "        #m = re.search(p, text, re.I)\n",
    "        #p = ' '.join(p.split())\n",
    "        m = re.search(p, text, re.I)\n",
    "        #print (p)\n",
    "        return list(m.span())\n",
    "    \n",
    "    else:\n",
    "        if row[actor][-1] in ['(', ')', ']','[']:\n",
    "            p = r'\\b' + row[actor].replace('(', \"\\(\").replace(')', \"\\)\").replace('[',\"\\[\").replace(']',\"\\]\")\n",
    "        else:\n",
    "            p = r'\\b' + row[actor].replace('(', \"\\(\").replace(')', \"\\)\").replace('[',\"\\[\").replace(']',\"\\]\") + r'\\b'\n",
    "            \n",
    "        actor_pos = [[tok.start(), tok.end()] for tok in re.finditer(p,text,re.I)]\n",
    "\n",
    "        hint_pos = np.array([(re.search(ht,text,re.I).span()) for ht in hints if len(re.findall(ht,text,re.I)) > 0])\n",
    "#         print ('actor_pos', actor_pos)\n",
    "#         print ('hint pos', hint_pos)\n",
    "        dist = cdist(actor_pos, hint_pos)\n",
    "#         print ('dist', dist)\n",
    "        return actor_pos[np.argmin(dist)]\n",
    "    \n",
    "\n",
    "# Above function just returns the span of occurance of the actor in the text. Where it starts and where it ends\n",
    "# give it a dataframe and actor column name, it will add four new columns start_actor, end_actor, start_hint, end_hint\n",
    "# usage ex:\n",
    "# tmp[['start_actor','end_actor']] = tmp.apply(get_span, args=['actor'], axis = 1, result_type = 'expand')\n",
    "# tmp[['start_hint','end_hint']] = tmp.apply(get_span, args=['hint'], axis = 1, result_type = 'expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to extract the statement category (p1,p2, p3,p4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cat(ac,text):\n",
    "    \n",
    "    \"\"\"\n",
    "    ac: list of actors\n",
    "    text: paragraph text\n",
    "    \n",
    "    returns the statement category (p1, p2, p3, p4)\n",
    "    \"\"\"\n",
    "    \n",
    "    broken_list_start = re.compile(r\"([(][a-zA-Z0-9][)])|([a-zA-Z0-9]\\.)\")\n",
    "    para = text.lower()\n",
    "    all_comb = [] # all combination of actor and cv\n",
    "    ht = ['shall', 'will', 'must', 'is required to', 'are required to']\n",
    "\n",
    "    if len(ac) == 0:\n",
    "        new_cat = 'p2'\n",
    "    else:\n",
    "        for x,y in list(itertools.product(ac, ht)):\n",
    "            all_comb.append(x.replace(' ', '') + y.replace(' ', ''))\n",
    "            if (para[-1] == ':') and (re.match(broken_list_start, para) is not None):\n",
    "                new_cat = 'p5'\n",
    "            elif any(' '.join(x.lower().split()) not in ' '.join(para.lower().split()) for x in ac):\n",
    "            #elif any(x.lower() not in para for x in ac):\n",
    "                new_cat = 'p4'\n",
    "            elif len(all_comb) > 0:\n",
    "                if any(comb.lower() in para.replace(' ', '') for comb in all_comb):\n",
    "                    new_cat = 'p1'\n",
    "                else:\n",
    "                    new_cat = 'p3'\n",
    "            else:\n",
    "                new_cat = 'p6'\n",
    "    return new_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funtion to convert the data into BIO format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bio(df, tag_ls):\n",
    "    \"\"\"\n",
    "    df: dataframe\n",
    "    tag_ls: list of column names for which the tagging needs to be done ex; ['actor']. span should have already been calculated.\n",
    "    \n",
    "    returns bio tagged data\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in df.index:\n",
    "        this_row = df.loc[i]\n",
    "        pid = this_row['id']\n",
    "\n",
    "        span_start, span_end = zip(*list(twt().span_tokenize(this_row['text'])))\n",
    "        token, pos = zip(*nltk.pos_tag(twt().tokenize(this_row['text'])))\n",
    "        token_df = pd.DataFrame({'word': token,\n",
    "                                 'pos': pos,\n",
    "                                'w_start':span_start,\n",
    "                                'w_end':span_end})   \n",
    "        #print (token_df)\n",
    "        \n",
    "        for name in tag_ls:\n",
    "            span_tag = df[df.id == pid][['start_'+name,'end_'+name, name]]\n",
    "            #print (span_tag)\n",
    "            token_df = token_df.merge(span_tag, how = 'left', left_on='w_start',right_on = 'start_'+name)\n",
    "            #print (token_df)\n",
    "            for k in token_df[token_df['start_'+name].notnull()].index:\n",
    "                token_df.at[k,name] = 'B-' + name\n",
    "                end = token_df.at[k,'end_'+name]\n",
    "                while token_df.at[k,'w_end'] < end :\n",
    "                    k += 1\n",
    "                    token_df.at[k,name] = 'I-' + name\n",
    "        \n",
    "        token_df = token_df[['word','pos']+tag_ls].fillna('O')\n",
    "        \n",
    "        result.append(token_df)\n",
    "        \n",
    "    return result  \n",
    "\n",
    "# Above function takes in df, list of actor, and hint columns. \n",
    "# returns list where each element is the text paragraph where each word is tagged in bio-format\n",
    "# [           word  pos    actor    hint\n",
    "#  0          DoDD  NNP        O       O\n",
    "#  1      4165.50E   CD        O       O\n",
    "#  2             ,    ,        O       O\n",
    "#  3      February  NNP        O       O\n",
    "#  4             7   CD        O       O\n",
    "#  5             ,    ,        O       O\n",
    "#  6          2014   CD        O       O\n",
    "#  7            b.   NN        O       O\n",
    "\n",
    "\n",
    "# usage ex:\n",
    "# tmp.reset_index(inplace=True)\n",
    "# tmp_result = reprot2bio(tmp, ['actor','hint'])\n",
    "\n",
    "# the above list of each paragrah tagged wordds in the text, can be converted into a dataframe\n",
    "\n",
    "# tmp_concat = pd.concat(tmp_result, keys =tmp.id.to_list())  . Here the id is the paragraphd id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to split the data into train, dev, test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_dev_split(pmid, p_train = 0.7, p_test = 0.15, r1 = 42, r2 = 100):\n",
    "    \"\"\"\n",
    "    split the id list into trian test dev list\n",
    "\n",
    "    pmid: list list of id for this func\n",
    "    p_train: train set per\n",
    "    p_test: test set per\n",
    "    r1: random state for first split\n",
    "    r2: random state for second split\n",
    "\n",
    "    return:\n",
    "        train_pmid: list for train\n",
    "        test_pmid: list for test\n",
    "        dev_pmid: list for dev\n",
    "    \"\"\"\n",
    "    train_pmid, other = train_test_split(pmid, test_size = (1 - p_train), random_state = r1)\n",
    "    test_pmid, dev_pmid = train_test_split(other, test_size = p_test/(1 - p_train), random_state = r2)\n",
    "    return train_pmid, test_pmid, dev_pmid\n",
    "\n",
    "# This function just randomly splits the data into training, dev, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to Extract Actors from the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_actors(stext, model_name):\n",
    "    ss = Sentence(stext)\n",
    "    model_name.predict(ss, all_tag_prob = True)\n",
    "    \n",
    "    ner = ss.to_dict(tag_type='ner')\n",
    "    tokens = [elem['text'] for elem in ner['entities']]\n",
    "    confidence = [elem['confidence'] for elem in ner['entities']]\n",
    "     \n",
    "    if len(tokens) == 0:\n",
    "        scores = []\n",
    "        for token in ss:\n",
    "            temp = {idx: token.tags_proba_dist['ner'][idx].score for idx in range(1,4)}\n",
    "            scores.append(max(temp.values()))\n",
    "        confidence = [min(scores)]\n",
    "    \n",
    "    return (tokens, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for comparing tagged actors and predicted actors for a match**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_match(a1,a2):\n",
    "    # a1,a2 are list of actors for tagged actors, and predicted actors respectively\n",
    "    \n",
    "    if(len(a1) != len(a2)):\n",
    "        return False\n",
    "    else:\n",
    "        a1 = ' '.join(a1).lower()\n",
    "        a2 = ' '.join(a2).lower()\n",
    "        a1 = re.sub('[^a-zA-Z0-9]', ' ', a1).strip()    # if we want to compare tagged vs. predicted by removing punctuation\n",
    "        a2 = re.sub('[^a-zA-Z0-9]', ' ', a2).strip()    # if we want to compare tagged vs. predicted by removing punctuation\n",
    "        return a1 == a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
